{"agent": "dqn", "states": {"gymtpl0": {"type": "float", "shape": [7], "min_value": -Infinity, "max_value": Infinity}, "gymtpl1": {"type": "int", "shape": [1]}}, "actions": {"type": "int", "shape": [], "num_values": 6}, "memory": 101, "batch_size": 64, "max_episode_timesteps": 100, "network": {"type": "custom", "layers": [{"type": "retrieve", "tensors": ["gymtpl0"]}, {"type": "linear_normalization"}, {"type": "dense", "bias": true, "activation": "tanh", "size": 64}, {"type": "dense", "bias": true, "activation": "tanh", "size": 64}, {"type": "register", "tensor": "gymtpl0-embeddings"}]}, "update_frequency": 0.25, "start_updating": null, "learning_rate": 0.001, "huber_loss": null, "horizon": 1, "discount": 0.99, "return_processing": null, "predict_terminal_values": false, "target_update_weight": 1.0, "target_sync_frequency": 1, "state_preprocessing": "linear_normalization", "reward_preprocessing": null, "exploration": 0.0, "variable_noise": 0.0, "l2_regularization": 0.0, "entropy_regularization": 0.0, "parallel_interactions": 1, "config": null, "saver": {"directory": "model"}, "summarizer": {"directory": "summaries", "summaries": ["reward", "graph"]}, "tracking": null, "recorder": null, "internals": {}, "initial_internals": {"policy": {}, "baseline": {}}}